---
title: "Metodologia"
---

Przy wyznaczaniu tematów uwzględniono tylko te słowa, które znalazły się wśród 3% wyrazów o największej ilości wystąpień w danym miesiącu oraz były kluczowe, tzn. ich statystyka Dunninga wyniosła co najmniej 50. Dokładna miara kluczowości statystycznej [Dunninga (1993)](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.5962) została podana w pierwszej tabeli. Przy wyliczaniu tej miary częstość występowania słowa w danym miesiącu porównywana jest z częstotliwością w okresie od 1 stycznia 2018 do 31 grudnia 2019. Wysoka wartość statystyki oznacza, że dane słowo pojawiło się w artykułach w badanym miesiącu częściej niż w okresie referencyjnym.

Dla wszystkich słów, które wystąpiły w badanym miesiącu, wyznaczono **embeddingi**, czyli wektorową reprezentację słów. Uzyskano je za pomocą dekompozycji (**SVD**) macierzy występowania słów (*TF matrix*) w akapitach i artykułach. W analizie tekstu taki sposób redukcji wymiarów macierzy *TF* nazywany jest *Latent Semantic Analysis* (**LSA**). Procedura ta jest analogiczna do analizy głównych składowych (*PCA*), ale nie wymaga standaryzacji poszczególnych kolumn macierzy *TF*. Dzięki temu w algorytmie dekompozycji można wykorzystać macierze rzadkie, czyli takie, które przechowują jedynie niezerowe informacje. Redukcja wymiarów pozwala na zmniejszenie liczby kolumn z około 140 tysięcy do zaledwie 512, przy jednoczesnym zachowaniu informacji o współwystępowaniu poszczególnych słów. 

Poszczególne słowa zostały automatycznie pogrupowane w tematy na podstawie podobieństwa cosinusowego między ich embeddingami. W tym celu wykorzystano algromeracyjne grupowanie hierarchiczne. Optymalną liczbę tematów wyznaczono przy pomocy algorytmu [silhouette](https://en.wikipedia.org/wiki/Silhouette_(clustering)). Embedding nowo powstałej grupy wyznaczano jako ważoną sumę embeddingów słów z danej grupy, gdzie wagą jest pierwiastek istotności słowa (statystyki Dunninga).

Zdania, które podsumowują poszczególne tematy zostały wybrane za pomocą zmodyfikowanego algorytmu [LexRank](https://blog.nus.edu.sg/soctalent/2010/02/11/a-brief-summary-of-lexrank-graph-based-lexical-centrality-as-salience-in-text-summarization/) opisanego w [artykule](https://pdfs.semanticscholar.org/44fc/a068eecce2203d111213e3691647914a3945.pdf) z 2004 r. oraz za pomocą podobieństwa cosinusowego między *embeddingiem* zdania a *embeddingiem* tematu.

Pełny opis metodologii znajduje się w [prezentacji](https://jkubajek.github.io/News_Selector/News_Selector.pdf).

Kod modelu wykorzystanego do stworzenia tego podsumowania można znaleźć na [Githubie](https://github.com/jkubajek/News_Selector).
